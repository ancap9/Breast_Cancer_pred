\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Breast Cancer Prediction},
            pdfauthor={Antonio Caputo},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Breast Cancer Prediction}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Antonio Caputo}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{26/09/2019}


\begin{document}
\maketitle

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

This project is about one of the most important problem in today
society: cancer disease prediction. With big data and Machine Learning
growth in biomedical and healthcare communities, accurate analysis of
medical data can predict early disease detection. Breast cancer is most
common form of cancer in Women. It represents about 12\% of all new
cancer cases and 25\% of all cancers in women. Here we build Machine
Learning Models to predict the type of Breast Cancer (Malignant or
Benign).

In order to build our model we use a study from US state of Wisconsin
the Breast Cancer Wisconsin (Diagnostic) DataSet
(\url{https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data}).The
data used for this project was collected in 1993 by the University of
Wisconsin and it is composed by the biopsy result of 569 patients in
Wisconsin Hospital. 3-D digitized images of breast mass(lump) were used
for extracting information. The dataset's features describe
characteristics of the cell nuclei on the image. The features
information are specified below:

\begin{itemize}
\tightlist
\item
  ID number
\item
  Diagnosis (M = malignant, B = benign)
\end{itemize}

and then we have the 10 variables used for the prediction:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  radius: mean of distances from center to points on the perimeter
\item
  texture: standard deviation of grey-scale values
\item
  perimeter
\item
  area: Number of pixels inside contour + Â½ for pixels on perimeter
\item
  smoothness: local variation in radius lengths
\item
  compactness: perimeter\^{}2 / area\\
\item
  concavity: severity of concave portions of the contour
\item
  concave points: number of concave portions of the contour
\item
  symmetry
\item
  fractal dimension: ``coastline approximation''
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(tidyverse)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(caret)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"caret"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(data.table)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"data.table"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(randomForest)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"randomForest"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(dplyr)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"dplyr"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(ggplot2)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(corrplot)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"corrplot"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{)}
\ControlFlowTok{if}\NormalTok{(}\OperatorTok{!}\KeywordTok{require}\NormalTok{(gam)) }\KeywordTok{install.packages}\NormalTok{(}\StringTok{"gam"}\NormalTok{, }\DataTypeTok{repos =} \StringTok{"http://cran.us.r-project.org"}\NormalTok{)}


\NormalTok{data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"}\NormalTok{)}
\KeywordTok{colnames}\NormalTok{(data)<-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"id"}\NormalTok{,}\StringTok{"diagnosis"}\NormalTok{,}\StringTok{"radius_mean"}\NormalTok{,}\StringTok{"texture_mean"}\NormalTok{,}\StringTok{"perimeter_mean"}\NormalTok{,}\StringTok{"area_mean"}\NormalTok{,}\StringTok{"smoothness_mean"}\NormalTok{,}
                   \StringTok{"compactness_mean"}\NormalTok{,}\StringTok{"concavity_mean"}\NormalTok{,}\StringTok{"concave points_mean"}\NormalTok{,}\StringTok{"symmetry_mean"}\NormalTok{,}\StringTok{"fractal_dimension_mean"}\NormalTok{,}\StringTok{"radius_se"}\NormalTok{,}\StringTok{"texture_se"}\NormalTok{,}\StringTok{"perimeter_se"}\NormalTok{,}\StringTok{"area_se"}\NormalTok{,}
                   \StringTok{"smoothness_se"}\NormalTok{,}\StringTok{"compactness_se"}\NormalTok{,}\StringTok{"concavity_se"}\NormalTok{,}\StringTok{"concave points_se"}\NormalTok{,}\StringTok{"symmetry_se"}\NormalTok{,}\StringTok{"fractal_dimension_se"}\NormalTok{,}\StringTok{"radius_worst"}\NormalTok{,}\StringTok{"texture_worst"}\NormalTok{,}\StringTok{"perimeter_worst"}\NormalTok{,}\StringTok{"area_worst"}\NormalTok{,}
                   \StringTok{"smoothness_worst"}\NormalTok{,}\StringTok{"compactness_worst"}\NormalTok{,}\StringTok{"concavity_worst"}\NormalTok{,}\StringTok{"concave points_worst"}\NormalTok{,}\StringTok{"symmetry_worst"}\NormalTok{,}\StringTok{"fractal_dimension_worst"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we have the mean, standard error and ``worst'' or largest (mean of
the three largest values) of these features were computed for each
image, resulting in 30 variables

This project will make a performance comparison between different
machine learning algorithms in order to to assess the correctness in
classifying data with respect to efficiency and effectiveness of each
algorithm in terms of accuracy, precision, sensitivity and specificity,
in order to find the best diagnosis.

\hypertarget{methodanalysis}{%
\section{Method/Analysis}\label{methodanalysis}}

Let's analize our data, dimension and type.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{class}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "data.frame"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 568  32
\end{verbatim}

So we have 568 observations, where \texttt{211} are malignant and
\texttt{357} are benign. So the distrubution of the observations is not
balanced.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(data, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x=}\NormalTok{diagnosis))}\OperatorTok{+}\KeywordTok{geom_bar}\NormalTok{(}\DataTypeTok{fill=}\StringTok{"red"}\NormalTok{,}\DataTypeTok{alpha=}\FloatTok{0.5}\NormalTok{)}\OperatorTok{+}\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title=}\StringTok{"Distribution"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{BC_pred_report_files/figure-latex/unnamed-chunk-3-1.pdf}

In our data set thera are 30 features, so we have to try to lighten it,
in order to have a lighter model. What we can do is see if there are any
features with high correlation that give us just redundant information.

So we build the correlation Matrix and find the features with
correletion more than 0.9 and than we remove them from our data set.

We also can see a corrplot for better understand which column are more
correlated:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{correlationMatrix <-}\StringTok{ }\KeywordTok{cor}\NormalTok{(data[,}\DecValTok{3}\OperatorTok{:}\KeywordTok{ncol}\NormalTok{(data)])}


\KeywordTok{corrplot}\NormalTok{(correlationMatrix)}
\end{Highlighting}
\end{Shaded}

\includegraphics{BC_pred_report_files/figure-latex/unnamed-chunk-4-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# find features that are highly corrected (corr >0.90)}
\NormalTok{high_correlation <-}\StringTok{ }\KeywordTok{findCorrelation}\NormalTok{(correlationMatrix, }\DataTypeTok{cutoff=}\FloatTok{0.9}\NormalTok{)}


\CommentTok{# print indexes of highly correlated attributes}
\KeywordTok{print}\NormalTok{(high_correlation)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1]  7  8 23 21  3 24  1 13 14  2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Remove correlated variables}

\NormalTok{data_no_cor <-}\StringTok{ }\NormalTok{data }\OperatorTok{%>%}\StringTok{ }\KeywordTok{select}\NormalTok{(}\OperatorTok{-}\NormalTok{high_correlation)}


\CommentTok{#final data set}

\NormalTok{data <-}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DataTypeTok{diagnosis=}\NormalTok{ data}\OperatorTok{$}\NormalTok{diagnosis, data_no_cor)}
\end{Highlighting}
\end{Shaded}

In the end our attributes will be 22

Before we start our analysis, we have to do the last step, which consist
of creatring the train and test set from our data. We split the dataset
into Train (70\%) and Test (30\%)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# split data in train and test set}
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DataTypeTok{sample.kind=}\StringTok{"Rounding"}\NormalTok{)}

\NormalTok{data_index <-}\StringTok{ }\KeywordTok{createDataPartition}\NormalTok{(data}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{times=}\DecValTok{1}\NormalTok{, }\DataTypeTok{p=}\FloatTok{0.7}\NormalTok{, }\DataTypeTok{list =} \OtherTok{FALSE}\NormalTok{)}
\NormalTok{train_set <-}\StringTok{ }\NormalTok{data[data_index, ]}
\NormalTok{test_set <-}\StringTok{ }\NormalTok{data[}\OperatorTok{-}\NormalTok{data_index, ]}

\NormalTok{fitControl <-}\StringTok{ }\KeywordTok{trainControl}\NormalTok{(}\DataTypeTok{method=}\StringTok{"cv"}\NormalTok{, }\DataTypeTok{number =} \DecValTok{15}\NormalTok{, }
                           \DataTypeTok{classProbs =} \OtherTok{TRUE}\NormalTok{,}\DataTypeTok{summaryFunction =}\NormalTok{ twoClassSummary)}
\end{Highlighting}
\end{Shaded}

\hypertarget{result}{%
\section{Result}\label{result}}

In this project we will now train different model to predict if the
observation is Malignant or Benign, and we will compare them using
different coefficent like accuracy, sensitivity, specificity, F1 score.

\hypertarget{logistic-regression-model}{%
\subsection{Logistic Regression Model}\label{logistic-regression-model}}

Logistic regression is a specific case of a set of generalized linear
models. Below we can see the results of this model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logr_model<-}\StringTok{ }\KeywordTok{train}\NormalTok{(diagnosis }\OperatorTok{~}\StringTok{ }\NormalTok{. ,}\DataTypeTok{data=}\NormalTok{ train_set, }\DataTypeTok{method=} \StringTok{"glm"}\NormalTok{,}
                   \DataTypeTok{preProcess=}\KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}\DataTypeTok{metric=}\StringTok{"ROC"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{ fitControl)}

\NormalTok{logr_pred<-}\StringTok{ }\KeywordTok{predict}\NormalTok{(logr_model, test_set)}

\NormalTok{logr_confmatrix<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(logr_pred, test_set}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{positive =} \StringTok{"M"}\NormalTok{)}

\NormalTok{logr_confmatrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction   B   M
         B 105   7
         M   2  56
                                          
               Accuracy : 0.9471          
                 95% CI : (0.9019, 0.9755)
    No Information Rate : 0.6294          
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.8846          
                                          
 Mcnemar's Test P-Value : 0.1824          
                                          
            Sensitivity : 0.8889          
            Specificity : 0.9813          
         Pos Pred Value : 0.9655          
         Neg Pred Value : 0.9375          
             Prevalence : 0.3706          
         Detection Rate : 0.3294          
   Detection Prevalence : 0.3412          
      Balanced Accuracy : 0.9351          
                                          
       'Positive' Class : M               
                                          
\end{verbatim}

And a plot of the most important variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{varImp}\NormalTok{(logr_model), }\DataTypeTok{top=}\DecValTok{10}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Top variables - Log Regr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{BC_pred_report_files/figure-latex/unnamed-chunk-7-1.pdf}

\hypertarget{k-nearest-neighbor}{%
\subsection{k-Nearest Neighbor}\label{k-nearest-neighbor}}

With k-nearest neighbors (kNN) we estimate p(x1, x2) in a similar way to
bin smoothing. However, as we will see, kNN is easier to adapt to
multiple dimensions. First we define the distance between all
observations based on the features. Then, for any point (x1, x2) for
which we want an estimate of p(x1, x2), we look for the k nearest points
to (x1, x2) and then take an average.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#knn}

\NormalTok{knn_model<-}\StringTok{ }\KeywordTok{train}\NormalTok{(diagnosis }\OperatorTok{~}\StringTok{ }\NormalTok{. ,}\DataTypeTok{data=}\NormalTok{ train_set, }\DataTypeTok{method=} \StringTok{"knn"}\NormalTok{,}
                 \DataTypeTok{preProcess=}\KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}\DataTypeTok{metric=}\StringTok{"ROC"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{ fitControl,}
                 \DataTypeTok{tuneGrid =} \KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{k =} \KeywordTok{seq}\NormalTok{(}\DecValTok{9}\NormalTok{, }\DecValTok{71}\NormalTok{, }\DecValTok{2}\NormalTok{)))}

\NormalTok{knn_pred<-}\StringTok{ }\KeywordTok{predict}\NormalTok{(knn_model, test_set)}

\NormalTok{knn_confmatrix<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(knn_pred, test_set}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{positive =} \StringTok{"M"}\NormalTok{)}

\NormalTok{knn_confmatrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction   B   M
         B 103   2
         M   4  61
                                          
               Accuracy : 0.9647          
                 95% CI : (0.9248, 0.9869)
    No Information Rate : 0.6294          
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.9248          
                                          
 Mcnemar's Test P-Value : 0.6831          
                                          
            Sensitivity : 0.9683          
            Specificity : 0.9626          
         Pos Pred Value : 0.9385          
         Neg Pred Value : 0.9810          
             Prevalence : 0.3706          
         Detection Rate : 0.3588          
   Detection Prevalence : 0.3824          
      Balanced Accuracy : 0.9654          
                                          
       'Positive' Class : M               
                                          
\end{verbatim}

\hypertarget{random-forest-model}{%
\subsection{Random Forest Model}\label{random-forest-model}}

Random forest is a method based on decision trees. It split data into
sub-samples, trains decision tree classifiers on each sub-sample and
averages prediction of each classifier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf_model<-}\StringTok{ }\KeywordTok{train}\NormalTok{(diagnosis }\OperatorTok{~}\StringTok{ }\NormalTok{. ,}\DataTypeTok{data=}\NormalTok{ train_set, }\DataTypeTok{method=} \StringTok{"rf"}\NormalTok{,}
                   \DataTypeTok{preProcess=}\KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{),}\DataTypeTok{metric=}\StringTok{"ROC"}\NormalTok{, }\DataTypeTok{trControl=}\NormalTok{ fitControl)}

\NormalTok{rf_pred<-}\StringTok{ }\KeywordTok{predict}\NormalTok{(rf_model, test_set)}

\NormalTok{rf_confmatrix<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(rf_pred, test_set}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{positive =} \StringTok{"M"}\NormalTok{)}

\NormalTok{rf_confmatrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction   B   M
         B 103   1
         M   4  62
                                          
               Accuracy : 0.9706          
                 95% CI : (0.9327, 0.9904)
    No Information Rate : 0.6294          
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.9376          
                                          
 Mcnemar's Test P-Value : 0.3711          
                                          
            Sensitivity : 0.9841          
            Specificity : 0.9626          
         Pos Pred Value : 0.9394          
         Neg Pred Value : 0.9904          
             Prevalence : 0.3706          
         Detection Rate : 0.3647          
   Detection Prevalence : 0.3882          
      Balanced Accuracy : 0.9734          
                                          
       'Positive' Class : M               
                                          
\end{verbatim}

And a plot of the most important variable:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(}\KeywordTok{varImp}\NormalTok{(rf_model), }\DataTypeTok{top=}\DecValTok{10}\NormalTok{, }\DataTypeTok{main=}\StringTok{"Top variables - Log Regr"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{BC_pred_report_files/figure-latex/unnamed-chunk-10-1.pdf}

We can see the plot with the two most important predictor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\OperatorTok{%>%}\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(perimeter_worst, area_worst, }\DataTypeTok{col=}\NormalTok{data}\OperatorTok{$}\NormalTok{diagnosis))}\OperatorTok{+}\StringTok{ }\KeywordTok{geom_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{BC_pred_report_files/figure-latex/unnamed-chunk-11-1.pdf}

\hypertarget{quadratic-discriminant-analysis-qda}{%
\subsection{Quadratic Discriminant Analysis
QDA}\label{quadratic-discriminant-analysis-qda}}

Quadratic Discriminant Analysis (QDA) is a version of Naive Bayes in
which we assume that the distributions pX\textbar Y =1(x) and
pX\textbar Y =0(x) are multivariate normal.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#qda}

\NormalTok{qda_model<-}\StringTok{ }\KeywordTok{train}\NormalTok{(diagnosis }\OperatorTok{~}\StringTok{ }\NormalTok{. ,}\DataTypeTok{data=}\NormalTok{ train_set, }\DataTypeTok{method=} \StringTok{"qda"}\NormalTok{,}
                  \DataTypeTok{preProcess=}\KeywordTok{c}\NormalTok{(}\StringTok{"scale"}\NormalTok{, }\StringTok{"center"}\NormalTok{))}

\NormalTok{qda_pred<-}\StringTok{ }\KeywordTok{predict}\NormalTok{(qda_model, test_set)}

\NormalTok{qda_confmatrix<-}\StringTok{ }\KeywordTok{confusionMatrix}\NormalTok{(qda_pred, test_set}\OperatorTok{$}\NormalTok{diagnosis, }\DataTypeTok{positive =} \StringTok{"M"}\NormalTok{)}

\NormalTok{qda_confmatrix}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Confusion Matrix and Statistics

          Reference
Prediction   B   M
         B 103   2
         M   4  61
                                          
               Accuracy : 0.9647          
                 95% CI : (0.9248, 0.9869)
    No Information Rate : 0.6294          
    P-Value [Acc > NIR] : <2e-16          
                                          
                  Kappa : 0.9248          
                                          
 Mcnemar's Test P-Value : 0.6831          
                                          
            Sensitivity : 0.9683          
            Specificity : 0.9626          
         Pos Pred Value : 0.9385          
         Neg Pred Value : 0.9810          
             Prevalence : 0.3706          
         Detection Rate : 0.3588          
   Detection Prevalence : 0.3824          
      Balanced Accuracy : 0.9654          
                                          
       'Positive' Class : M               
                                          
\end{verbatim}

\hypertarget{conclusion}{%
\section{Conclusion}\label{conclusion}}

We trained 4 model: logistic regression, Random Forest, k-Nearest
Neighbor and Quadratic Discriminant Analysis. Let's see now our results
and difference. Below we have summary table for every model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{models<-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{Log_regr=}\NormalTok{ logr_model, }\DataTypeTok{Random_Forest=}\NormalTok{ rf_model, }\DataTypeTok{KNN=}\NormalTok{ knn_model,}\DataTypeTok{QDA=}\NormalTok{qda_model)}

\NormalTok{confusionmatrix<-}\StringTok{ }\KeywordTok{list}\NormalTok{(}\DataTypeTok{Log_regr=}\NormalTok{ logr_confmatrix, }\DataTypeTok{Random_Forest=}\NormalTok{ rf_confmatrix,}
                       \DataTypeTok{KNN=}\NormalTok{ knn_confmatrix,}\DataTypeTok{QDA=}\NormalTok{qda_confmatrix)}

\NormalTok{confusionmatrix_results <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(confusionmatrix, }\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{$}\NormalTok{byClass)}
\NormalTok{confusionmatrix_results }\OperatorTok{%>%}\StringTok{ }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}
\toprule
& Log\_regr & Random\_Forest & KNN & QDA\tabularnewline
\midrule
\endhead
Sensitivity & 0.8888889 & 0.9841270 & 0.9682540 &
0.9682540\tabularnewline
Specificity & 0.9813084 & 0.9626168 & 0.9626168 &
0.9626168\tabularnewline
Pos Pred Value & 0.9655172 & 0.9393939 & 0.9384615 &
0.9384615\tabularnewline
Neg Pred Value & 0.9375000 & 0.9903846 & 0.9809524 &
0.9809524\tabularnewline
Precision & 0.9655172 & 0.9393939 & 0.9384615 & 0.9384615\tabularnewline
Recall & 0.8888889 & 0.9841270 & 0.9682540 & 0.9682540\tabularnewline
F1 & 0.9256198 & 0.9612403 & 0.9531250 & 0.9531250\tabularnewline
Prevalence & 0.3705882 & 0.3705882 & 0.3705882 &
0.3705882\tabularnewline
Detection Rate & 0.3294118 & 0.3647059 & 0.3588235 &
0.3588235\tabularnewline
Detection Prevalence & 0.3411765 & 0.3882353 & 0.3823529 &
0.3823529\tabularnewline
Balanced Accuracy & 0.9350987 & 0.9733719 & 0.9654354 &
0.9654354\tabularnewline
\bottomrule
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{confusionmatrix_accuracy <-}\StringTok{ }\KeywordTok{list}\NormalTok{(confusionmatrix}\OperatorTok{$}\NormalTok{Log_regr}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{],}
\NormalTok{                                 confusionmatrix}\OperatorTok{$}\NormalTok{Random_Forest}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{],}
\NormalTok{                                 confusionmatrix}\OperatorTok{$}\NormalTok{KNN}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{],}
\NormalTok{                                 confusionmatrix}\OperatorTok{$}\NormalTok{QDA}\OperatorTok{$}\NormalTok{overall[}\StringTok{"Accuracy"}\NormalTok{])}
\NormalTok{confusionmatrix_accuracy }\OperatorTok{%>%}\StringTok{ }\NormalTok{knitr}\OperatorTok{::}\KeywordTok{kable}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{table}

\centering
\begin{tabular}[t]{l|r}
\hline
  & x\\
\hline
Accuracy & 0.9470588\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{l|r}
\hline
  & x\\
\hline
Accuracy & 0.9705882\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{l|r}
\hline
  & x\\
\hline
Accuracy & 0.9647059\\
\hline
\end{tabular}
\centering
\begin{tabular}[t]{l|r}
\hline
  & x\\
\hline
Accuracy & 0.9647059\\
\hline
\end{tabular}
\end{table}

We can see that random forest model perfom better, Although in term of
accuracy there isn't a lot of difference we notice more difference in
term of sensitivity. In this particular kind of problem we are more
interested and worried in predicting True Positives and so less False
Negative that have a bigger social cost.


\end{document}
